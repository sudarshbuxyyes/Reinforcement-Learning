{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-24T13:34:49.660384Z","iopub.execute_input":"2021-10-24T13:34:49.660655Z","iopub.status.idle":"2021-10-24T13:34:49.666178Z","shell.execute_reply.started":"2021-10-24T13:34:49.660625Z","shell.execute_reply":"2021-10-24T13:34:49.665199Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n!pip install matplotlib\nimport matplotlib \nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:34:49.668126Z","iopub.execute_input":"2021-10-24T13:34:49.668792Z","iopub.status.idle":"2021-10-24T13:34:56.882480Z","shell.execute_reply.started":"2021-10-24T13:34:49.668743Z","shell.execute_reply":"2021-10-24T13:34:56.881412Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"action_hit = 0\naction_strike = 1\nactions = [action_hit, action_strike]\nplayer_policy = np.zeros((22, 1), dtype=np.int)\n# for i in range(12, 20):\n#player_policy[i] = action_hit\n# print(player_policy)\nplayer_policy[20] = action_strike\nplayer_policy[21] = action_strike\n\n\ndef target_policy_player(usable_ace_player,player_sum, dealer_card):\n    return player_policy[player_sum]\n\n\ndef behaviour_policy_player():\n    if np.random.binomial(1, 0.5) == 1:\n        return action_strike\n    return action_hit\n\n\ndealer_policy = np.zeros(22)\nfor i in range(17, 22):\n    dealer_policy[i] = action_strike\n\n\ndef get_card():\n\n    card = np.minimum(np.random.randint(1, 14), 10)\n    return card\n\n\ndef card_value(card):\n    if card == 1:\n        return 11\n    else:\n        return card\n\n\ndef play(player_policy, initial_state=None, initial_action=None):\n    player_sum = 0\n    player_t = []\n    usable_ace_player = False\n\n    dealer_card1 = 0\n    dealer_card2 = 0\n    usable_ace_dealer = False\n\n    if initial_state is None:\n        while player_sum < 12:\n            card = get_card()\n            player_sum += card_value(card)\n            if player_sum > 21:\n                if player_sum == 22:\n                    player_sum -= 10\n            else:\n                usable_ace_player |= (1 == card)\n        dealer_card1 = get_card()\n        dealer_card2 = get_card()\n    else:\n        usable_ace_player, player_sum, dealer_card1 = initial_state\n        dealer_card2 = get_card()\n\n    state = [usable_ace_player, player_sum, dealer_card1]\n    dealer_sum = card_value(dealer_card1) + card_value(dealer_card2)\n    usable_ace_dealer = 1 in (dealer_card1, dealer_card2)\n    if dealer_sum > 21:\n        if dealer_sum == 22:\n            dealer_sum -= 10\n    assert dealer_sum <= 21\n    assert player_sum <= 21\n\n    while True:\n        if initial_action != None:\n            action = initial_action\n            initial_action = None\n        else:\n            action = player_policy(usable_ace_player, player_sum, dealer_card1)\n\n        player_t.append(\n            [(usable_ace_player, player_sum, dealer_card1), action])\n        if action == action_strike:\n            break\n        card = get_card()\n        ace_count = usable_ace_player\n        if card == 1:\n            ace_count += 1\n        player_sum += card_value(card)\n        while player_sum > 21 and ace_count:\n            player_sum -= 10\n            ace_count -= 1\n        if player_sum > 21:\n            return state, -1, player_t\n        assert player_sum <= 21\n        usable_ace_player = (ace_count == 1)\n\n    while True:\n        action = dealer_policy[dealer_sum]\n        if action == action_strike:\n            break\n        new_card = get_card()\n        ace_count = int(usable_ace_dealer)\n        if new_card == 1:\n            ace_count += 1\n        dealer_sum += card_value(new_card)\n        while dealer_sum > 21 and ace_count:\n            dealer_sum -= 10\n            ace_count -= 1\n        if dealer_sum > 21:\n            return state, 1, player_t\n        usable_ace_dealer = (ace_count == 1)\n\n    assert player_sum <= 21 and dealer_sum <= 21\n    return_ans = -1\n    if player_sum > dealer_sum:\n        return_ans = 1\n\n    elif player_sum == dealer_sum:\n        return_ans = 0\n\n    return state, return_ans, player_t\n\n\ndef monte_carlo_off_policy(episodes):\n    initial_state = [True, 13, 2]\n    rhos = []\n    returns = []\n    for i in range(0, episodes):\n        state, reward, player_trajectory = play(\n            behaviour_policy_player, initial_state=initial_state)\n        num = 1.0\n        den = 1.0\n        for (usable_ace, player_sum, dealer_card), action in player_trajectory:\n            if action == target_policy_player(usable_ace, player_sum, dealer_card):\n                den *= 0.5\n            else:\n                num = 0.0\n                break\n\n        rho = num/den\n        returns = np.asarray(rhos)\n        w_returns = np.multiply(rhos, returns)\n        w_returns = np.cumsum(w_returns)\n        rhos = np.cumsum(rhos)\n        ordinary_sampling = w_returns/np.arange(1, episodes + 1)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            weighted_sampling = np.where(rhos != 0, w_returns/rhos, 0)\n\n        return ordinary_sampling, weighted_sampling\n\n\ndef monte_carlo_es(episodes):\n    state_action_values = np.zeros((10, 10, 2, 2))\n    state_action_pair_count = np.ones((10, 10, 2, 2))\n\n    def behavior_policy(usable_ace, player_sum, dealer_card):\n        usable_ace = int(usable_ace)\n        player_sum -= 12\n        dealer_card -= 1\n        values_ = state_action_values[player_sum, dealer_card, usable_ace, :] / \\\n            state_action_pair_count[player_sum, dealer_card, usable_ace, :]\n        return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n\n\n    for episode in (range(episodes)):\n        initial_state = [bool(np.random.choice([0, 1])),\n                         np.random.choice(range(12, 22)),\n                         np.random.choice(range(1, 11))]\n        initial_action = np.random.choice(actions)\n        current_policy = behavior_policy if episode else target_policy_player\n        state, reward, trajectory = play(\n            current_policy, initial_state, initial_action)\n        first_visit_check = set()\n        for (usable_ace, player_sum, dealer_card), action in trajectory:\n            usable_ace = int(usable_ace)\n            player_sum -= 12\n            dealer_card -= 1\n            state_action = (usable_ace, player_sum, dealer_card, action)\n            if state_action in first_visit_check:\n                continue\n            first_visit_check.add(state_action)\n\n            state_action_values[player_sum, dealer_card,\n                                usable_ace, action] += reward\n            state_action_pair_count[player_sum,\n                                    dealer_card, usable_ace, action] += 1\n\n    return state_action_values / state_action_pair_count\n\n\ndef monte_carlo_on_policy(episodes):\n    states_usable_ace = np.zeros((10, 10))\n    states_usable_ace_count = np.ones((10, 10))\n    states_no_usable_ace = np.zeros((10, 10))\n    states_no_usable_ace_count = np.ones((10, 10))\n    for i in range(0, episodes):\n        state, reward, player_trajectory = play(target_policy_player)\n        for (usable_ace, player_sum, dealer_card), action in player_trajectory:\n            player_sum -= 12\n            dealer_card -= 1\n            if usable_ace:\n                states_usable_ace_count[player_sum, dealer_card] += 1\n                states_usable_ace[player_sum, dealer_card] += reward\n            else:\n                states_no_usable_ace_count[player_sum, dealer_card] += 1\n                states_no_usable_ace[player_sum, dealer_card] += reward\n    return states_usable_ace / states_usable_ace_count, states_no_usable_ace / states_no_usable_ace_count\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:34:56.886505Z","iopub.execute_input":"2021-10-24T13:34:56.886751Z","iopub.status.idle":"2021-10-24T13:34:56.918594Z","shell.execute_reply.started":"2021-10-24T13:34:56.886721Z","shell.execute_reply":"2021-10-24T13:34:56.917324Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"print(\"Generating Figure 5.1....\")\nstates_usable_ace_1, states_no_usable_ace_1 = monte_carlo_on_policy(10000)\nstates_usable_ace_2, states_no_usable_ace_2 = monte_carlo_on_policy(500000)\nstates = [states_usable_ace_1,states_usable_ace_2,states_no_usable_ace_1,states_no_usable_ace_2]\ntitles = ['Usable Ace, 10000 Episodes','Usable Ace, 500000 Episodes','No Usable Ace, 10000 Episodes','No Usable Ace, 500000 Episodes']\nx, axes = plt.subplots(2,2,figsize=(40,30))\nplt.subplots_adjust(wspace=0.1, hspace=0.2)\naxes = axes.flatten()\nfor state, title, axis in zip(states,titles,axes):\n    fig = sns.heatmap(np.flipud(state), cmap=\"YlGnBu\", ax=axis, xticklabels=range(1, 11),\n                          yticklabels=list(reversed(range(12, 22))))\n    fig.set_ylabel('player sum', fontsize=30)\n    fig.set_xlabel('dealer showing', fontsize=30)\n    fig.set_title(title, fontsize=30)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:34:56.920107Z","iopub.execute_input":"2021-10-24T13:34:56.920475Z","iopub.status.idle":"2021-10-24T13:35:20.928054Z","shell.execute_reply.started":"2021-10-24T13:34:56.920443Z","shell.execute_reply":"2021-10-24T13:35:20.927180Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:20.930412Z","iopub.execute_input":"2021-10-24T13:35:20.931321Z","iopub.status.idle":"2021-10-24T13:35:20.935982Z","shell.execute_reply.started":"2021-10-24T13:35:20.931272Z","shell.execute_reply":"2021-10-24T13:35:20.935222Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"Code for generating the figures in 6.2:","metadata":{}},{"cell_type":"code","source":"values = np.zeros(7)\nvalues[1:6] = 0.5\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:20.937399Z","iopub.execute_input":"2021-10-24T13:35:20.937730Z","iopub.status.idle":"2021-10-24T13:35:20.948525Z","shell.execute_reply.started":"2021-10-24T13:35:20.937686Z","shell.execute_reply":"2021-10-24T13:35:20.947550Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"0 is the left terminal state and 6 is the right terminal state.\nstates from 1 to 5 are non-terminal.","metadata":{}},{"cell_type":"code","source":"values[6] = 1\ntrue_value = np.zeros(7)\ntrue_value[1:6] = np.arange(1,6)/6.0\ntrue_value[6] = 1\nprint(true_value)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:20.949971Z","iopub.execute_input":"2021-10-24T13:35:20.950577Z","iopub.status.idle":"2021-10-24T13:35:20.963330Z","shell.execute_reply.started":"2021-10-24T13:35:20.950535Z","shell.execute_reply":"2021-10-24T13:35:20.962422Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"right = 0\nleft = 1","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:20.964582Z","iopub.execute_input":"2021-10-24T13:35:20.965248Z","iopub.status.idle":"2021-10-24T13:35:20.974561Z","shell.execute_reply.started":"2021-10-24T13:35:20.965195Z","shell.execute_reply":"2021-10-24T13:35:20.973852Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"def temporal_difference(values, alpha = 0.1, batch = False):\n    state = 3\n    trajectory = [state]\n    rewards = [0]\n    while True:\n        prev_state = state\n        if np.random.binomial(1,0.5)==left:\n            state-=1\n        else:\n            state+=1\n        reward = 0\n        trajectory.append(state)\n        if not batch:\n            values[prev_state]+= alpha* (reward+values[state]-values[prev_state])\n        if state == 6:\n           # print(\"Terminal state\")\n            break\n        if state ==0:\n            break\n        rewards.append(reward)\n    return trajectory,rewards","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:20.975603Z","iopub.execute_input":"2021-10-24T13:35:20.976272Z","iopub.status.idle":"2021-10-24T13:35:20.987659Z","shell.execute_reply.started":"2021-10-24T13:35:20.976240Z","shell.execute_reply":"2021-10-24T13:35:20.986548Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"def monte_carlo(vals, alpha = 0.1, batch = False):\n    state = 3\n    trajectory = [3]\n    while True:\n        if np.random.binomial(1,0.5)==left:\n            state-=1\n        else:\n            state+=1\n        trajectory.append(state)\n        if state == 6:\n            returns = 1.0\n            #print(\"Terminal state\")\n            break\n        elif state == 0:\n            returns = 0.0\n            break\n            \n    if batch == False:\n        for state in trajectory[:-1]:\n            values[state] += alpha * (returns - values[state])\n    return trajectory, [returns] * (len(trajectory)-1)\n\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:20.989002Z","iopub.execute_input":"2021-10-24T13:35:20.989449Z","iopub.status.idle":"2021-10-24T13:35:20.999119Z","shell.execute_reply.started":"2021-10-24T13:35:20.989418Z","shell.execute_reply":"2021-10-24T13:35:20.998253Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"def compute_state_value():\n    plt.figure(1)\n    episodes = [0,1,10,100]\n    current_values = np.copy(values)\n    for i in range(episodes[-1]+1):\n        if i in episodes:\n            plt.plot(current_values, label= str(i) + 'episodes')\n        temporal_difference(current_values)\n    plt.plot(true_value, label = 'true values')\n    plt.xlabel('state')\n    plt.ylabel('estimated values')\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:21.001381Z","iopub.execute_input":"2021-10-24T13:35:21.001843Z","iopub.status.idle":"2021-10-24T13:35:21.014395Z","shell.execute_reply.started":"2021-10-24T13:35:21.001811Z","shell.execute_reply":"2021-10-24T13:35:21.013714Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"td_alphas = [0.15,0.1,0.05]\nmc_alphas = [0.01,0.02,0.03,0.04]\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:21.015428Z","iopub.execute_input":"2021-10-24T13:35:21.016069Z","iopub.status.idle":"2021-10-24T13:35:21.027870Z","shell.execute_reply.started":"2021-10-24T13:35:21.016034Z","shell.execute_reply":"2021-10-24T13:35:21.026867Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"def rms_error(td_alphas, mc_alphas):\n    runs = 100\n    eps = 101\n    for i, alpha in enumerate(td_alphas + mc_alphas):\n        total_errors = np.zeros(eps)\n        if i < len(td_alphas):\n            method = 'Temporal Difference'\n            linestyle = 'solid'\n        else:\n            method = 'Monte Carlo'\n            linestyle = 'dashdot'\n        for r in range(runs):\n            errors = []\n            current_vals = np.copy(values)\n            for i in range(eps):\n                error = np.sqrt(np.sum(np.power(true_value-current_vals,2)))\n                errors.append(error)\n                if method == 'Temporal Difference':\n                    temporal_difference(current_vals, alpha = alpha)\n                else:\n                    monte_carlo(current_vals, alpha = alpha)\n            total_errors += np.asarray(errors)\n        total_errors /= runs\n        plt.plot(total_errors,linestyle = linestyle, label = method + ', alpha = %.02f' % alpha)\n    plt.xlabel('eps')\n    plt.ylabel('rms')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:21.029086Z","iopub.execute_input":"2021-10-24T13:35:21.029354Z","iopub.status.idle":"2021-10-24T13:35:21.040605Z","shell.execute_reply.started":"2021-10-24T13:35:21.029313Z","shell.execute_reply":"2021-10-24T13:35:21.039645Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,20))\nplt.subplot(2,1,1)\ncompute_state_value()\nplt.subplot(2,1,2)\nrms_error(td_alphas, mc_alphas)\nplt.tight_layout\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T13:35:35.721343Z","iopub.execute_input":"2021-10-24T13:35:35.721649Z","iopub.status.idle":"2021-10-24T13:35:39.406490Z","shell.execute_reply.started":"2021-10-24T13:35:35.721616Z","shell.execute_reply":"2021-10-24T13:35:39.405560Z"},"trusted":true},"execution_count":90,"outputs":[]}]}